#!/usr/bin/env python3
"""
Sci-Hubå¤§è§„æ¨¡æµ‹è¯•è„šæœ¬
ä»Google Scholaræœç´¢50ç¯‡PRBè®ºæ–‡ï¼Œåœ¨Sci-Hubä¸Šæµ‹è¯•ä¸‹è½½æˆåŠŸç‡
"""

import requests
import time
import logging
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import os
import json
from dataclasses import dataclass
from typing import List, Optional
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import undetected_chromedriver as uc

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PaperInfo:
    """è®ºæ–‡ä¿¡æ¯"""
    title: str
    authors: str
    journal: str
    year: str
    doi: str
    url: str

class GoogleScholarSearcher:
    """Google Scholaræœç´¢å™¨"""
    
    def __init__(self):
        self.driver = None
        self._setup_driver()
    
    def _setup_driver(self):
        """è®¾ç½®Seleniumé©±åŠ¨"""
        try:
            options = uc.ChromeOptions()
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-blink-features=AutomationControlled')
            
            self.driver = uc.Chrome(options=options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            logger.info("Google Scholaræœç´¢å™¨è®¾ç½®å®Œæˆ")
            
        except Exception as e:
            logger.error(f"Google Scholaræœç´¢å™¨è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def search_prb_papers(self, max_papers=50):
        """æœç´¢PRBè®ºæ–‡"""
        try:
            logger.info("å¼€å§‹æœç´¢PRBè®ºæ–‡")
            
            # è®¿é—®Google Scholar
            self.driver.get("https://scholar.google.com")
            time.sleep(3)
            
            # æœç´¢PRBè®ºæ–‡
            search_query = "site:journals.aps.org/prb single crystal growth"
            search_box = self.driver.find_element(By.NAME, "q")
            search_box.clear()
            search_box.send_keys(search_query)
            search_box.submit()
            
            time.sleep(3)
            
            papers = []
            page_count = 0
            max_pages = 10  # æœ€å¤šæœç´¢10é¡µ
            
            while len(papers) < max_papers and page_count < max_pages:
                logger.info(f"æœç´¢ç¬¬ {page_count + 1} é¡µ")
                
                # è§£ææœç´¢ç»“æœ
                page_papers = self._parse_search_results()
                papers.extend(page_papers)
                
                logger.info(f"æœ¬é¡µæ‰¾åˆ° {len(page_papers)} ç¯‡è®ºæ–‡ï¼Œæ€»è®¡ {len(papers)} ç¯‡")
                
                # å°è¯•ç¿»é¡µ
                if not self._go_to_next_page():
                    break
                
                page_count += 1
                time.sleep(2)
            
            # é™åˆ¶åˆ°æŒ‡å®šæ•°é‡
            papers = papers[:max_papers]
            logger.info(f"æ€»å…±æ‰¾åˆ° {len(papers)} ç¯‡PRBè®ºæ–‡")
            
            return papers
            
        except Exception as e:
            logger.error(f"æœç´¢PRBè®ºæ–‡å¤±è´¥: {e}")
            return []
    
    def _parse_search_results(self):
        """è§£ææœç´¢ç»“æœ"""
        papers = []
        
        try:
            # æŸ¥æ‰¾æœç´¢ç»“æœ
            result_elements = self.driver.find_elements(By.CSS_SELECTOR, "div.gs_ri")
            
            for element in result_elements:
                try:
                    # æå–æ ‡é¢˜
                    title_element = element.find_element(By.CSS_SELECTOR, "h3 a")
                    title = title_element.text
                    url = title_element.get_attribute("href")
                    
                    # æå–ä½œè€…å’ŒæœŸåˆŠä¿¡æ¯
                    author_info = element.find_element(By.CSS_SELECTOR, "div.gs_a").text
                    
                    # è§£æä½œè€…ä¿¡æ¯
                    parts = author_info.split(" - ")
                    authors = parts[0] if len(parts) > 0 else ""
                    journal_year = parts[1] if len(parts) > 1 else ""
                    
                    # æå–æœŸåˆŠå’Œå¹´ä»½
                    journal = ""
                    year = ""
                    if journal_year:
                        # æŸ¥æ‰¾å¹´ä»½
                        year_match = re.search(r'\b(19|20)\d{2}\b', journal_year)
                        if year_match:
                            year = year_match.group()
                            journal = journal_year.replace(year, "").strip()
                        else:
                            journal = journal_year
                    
                    # æå–DOI
                    doi = self._extract_doi_from_url(url)
                    
                    if doi:
                        paper = PaperInfo(
                            title=title,
                            authors=authors,
                            journal=journal,
                            year=year,
                            doi=doi,
                            url=url
                        )
                        papers.append(paper)
                        logger.info(f"æ‰¾åˆ°è®ºæ–‡: {title[:50]}...")
                    
                except Exception as e:
                    logger.warning(f"è§£æè®ºæ–‡ä¿¡æ¯å¤±è´¥: {e}")
                    continue
            
        except Exception as e:
            logger.error(f"è§£ææœç´¢ç»“æœå¤±è´¥: {e}")
        
        return papers
    
    def _extract_doi_from_url(self, url):
        """ä»URLä¸­æå–DOI"""
        try:
            if "journals.aps.org/prb/abstract/" in url:
                # ä»APS URLä¸­æå–DOI
                doi_match = re.search(r'/abstract/(10\.\d+/[^/]+)', url)
                if doi_match:
                    return doi_match.group(1)
            return None
        except:
            return None
    
    def _go_to_next_page(self):
        """ç¿»åˆ°ä¸‹ä¸€é¡µ"""
        try:
            next_button = self.driver.find_element(By.CSS_SELECTOR, "a[aria-label='Next']")
            if next_button.is_enabled():
                next_button.click()
                time.sleep(3)
                return True
            return False
        except:
            return False
    
    def close(self):
        """å…³é—­é©±åŠ¨"""
        if self.driver:
            self.driver.quit()

class SciHubTester:
    """Sci-Hubæµ‹è¯•å™¨"""
    
    def __init__(self, base_url="https://sci-hub.in/"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def test_paper_download(self, paper: PaperInfo):
        """æµ‹è¯•å•ç¯‡è®ºæ–‡ä¸‹è½½"""
        try:
            logger.info(f"æµ‹è¯•è®ºæ–‡: {paper.title[:50]}...")
            
            # æœç´¢è®ºæ–‡
            search_url = f"{self.base_url}{paper.doi}"
            logger.info(f"è®¿é—®URL: {search_url}")
            
            response = self.session.get(search_url, timeout=60)  # å¢åŠ è¶…æ—¶æ—¶é—´
            
            if response.status_code != 200:
                logger.warning(f"æœç´¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False
            
            logger.info("é¡µé¢åŠ è½½å®Œæˆï¼Œå¼€å§‹æå–PDFé“¾æ¥...")
            
            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            time.sleep(3)
            
            # æå–PDFé“¾æ¥
            pdf_url = self._extract_pdf_url(response.text)
            if not pdf_url:
                logger.warning("æœªæ‰¾åˆ°PDFé“¾æ¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
                # å°è¯•ä»é¡µé¢å†…å®¹ä¸­æœç´¢PDFé“¾æ¥
                pdf_url = self._search_pdf_in_content(response.text)
                if not pdf_url:
                    logger.warning("ä»æœªæ‰¾åˆ°PDFé“¾æ¥")
                    return False
            
            logger.info(f"æ‰¾åˆ°PDFé“¾æ¥: {pdf_url}")
            
            # æµ‹è¯•PDFè®¿é—®
            pdf_response = self.session.head(pdf_url, timeout=30)
            if pdf_response.status_code == 200:
                logger.info("âœ… PDFå¯è®¿é—®")
                return True
            else:
                logger.warning(f"PDFä¸å¯è®¿é—®ï¼ŒçŠ¶æ€ç : {pdf_response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def _extract_pdf_url(self, html_content):
        """ä»HTMLå†…å®¹ä¸­æå–PDFé“¾æ¥"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æŸ¥æ‰¾PDFé“¾æ¥
            pdf_selectors = [
                'a[href*=".pdf"]',
                'a[href*="pdf"]',
                'a[title*="PDF"]',
                'a[title*="pdf"]',
                '#pdf',
                '.pdf'
            ]
            
            for selector in pdf_selectors:
                try:
                    element = soup.select_one(selector)
                    if element and element.get('href'):
                        href = element['href']
                        if href.startswith('//'):
                            return 'https:' + href
                        elif href.startswith('/'):
                            return self.base_url.rstrip('/') + href
                        elif href.startswith('http'):
                            return href
                        else:
                            return urljoin(self.base_url, href)
                except:
                    continue
            
            return None
            
        except Exception as e:
            logger.error(f"PDFé“¾æ¥æå–å¤±è´¥: {e}")
            return None
    
    def _search_pdf_in_content(self, html_content):
        """ä»é¡µé¢å†…å®¹ä¸­æœç´¢PDFé“¾æ¥"""
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢PDFé“¾æ¥
            pdf_patterns = [
                r'href="([^"]*\.pdf[^"]*)"',
                r'src="([^"]*\.pdf[^"]*)"',
                r'url\(["\']?([^"\']*\.pdf[^"\']*)["\']?\)',
                r'https?://[^"\s]*\.pdf[^"\s]*',
                r'//[^"\s]*\.pdf[^"\s]*'
            ]
            
            for pattern in pdf_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                if matches:
                    pdf_url = matches[0]
                    if pdf_url.startswith('//'):
                        pdf_url = 'https:' + pdf_url
                    elif pdf_url.startswith('/'):
                        pdf_url = self.base_url.rstrip('/') + pdf_url
                    elif not pdf_url.startswith('http'):
                        pdf_url = urljoin(self.base_url, pdf_url)
                    
                    logger.info(f"é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ°PDFé“¾æ¥: {pdf_url}")
                    return pdf_url
            
            # æœç´¢åŒ…å«PDFçš„iframe
            iframe_pattern = r'<iframe[^>]*src="([^"]*)"[^>]*>'
            iframe_matches = re.findall(iframe_pattern, html_content, re.IGNORECASE)
            for iframe_src in iframe_matches:
                if '.pdf' in iframe_src.lower():
                    pdf_url = iframe_src
                    if pdf_url.startswith('//'):
                        pdf_url = 'https:' + pdf_url
                    elif pdf_url.startswith('/'):
                        pdf_url = self.base_url.rstrip('/') + pdf_url
                    elif not pdf_url.startswith('http'):
                        pdf_url = urljoin(self.base_url, pdf_url)
                    
                    logger.info(f"é€šè¿‡iframeæ‰¾åˆ°PDFé“¾æ¥: {pdf_url}")
                    return pdf_url
            
            return None
            
        except Exception as e:
            logger.error(f"PDFé“¾æ¥æœç´¢å¤±è´¥: {e}")
            return None

def test_scihub_large_scale():
    """å¤§è§„æ¨¡æµ‹è¯•Sci-Hub"""
    print("Sci-Hubå¤§è§„æ¨¡æµ‹è¯• - 50ç¯‡PRBè®ºæ–‡")
    print("=" * 60)
    
    # 1. æœç´¢PRBè®ºæ–‡
    print("\næ­¥éª¤1: ä»Google Scholaræœç´¢PRBè®ºæ–‡")
    print("-" * 40)
    
    searcher = GoogleScholarSearcher()
    try:
        papers = searcher.search_prb_papers(max_papers=50)
        print(f"âœ… æ‰¾åˆ° {len(papers)} ç¯‡PRBè®ºæ–‡")
        
        if len(papers) == 0:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡")
            return
        
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {e}")
        return
    finally:
        searcher.close()
    
    # 2. åœ¨Sci-Hubä¸Šæµ‹è¯•ä¸‹è½½
    print(f"\næ­¥éª¤2: åœ¨Sci-Hubä¸Šæµ‹è¯• {len(papers)} ç¯‡è®ºæ–‡")
    print("-" * 40)
    
    tester = SciHubTester()
    success_count = 0
    failed_papers = []
    
    for i, paper in enumerate(papers, 1):
        print(f"\næµ‹è¯• {i}/{len(papers)}: {paper.title[:60]}...")
        print(f"DOI: {paper.doi}")
        print(f"å¹´ä»½: {paper.year}")
        
        if tester.test_paper_download(paper):
            print("âœ… æˆåŠŸ")
            success_count += 1
        else:
            print("âŒ å¤±è´¥")
            failed_papers.append(paper)
        
        # é¿å…è¯·æ±‚è¿‡å¿«ï¼Œå¢åŠ å»¶è¿Ÿ
        time.sleep(3)
    
    # 3. è¾“å‡ºç»“æœç»Ÿè®¡
    print(f"\næµ‹è¯•ç»“æœç»Ÿè®¡")
    print("=" * 60)
    print(f"æ€»è®ºæ–‡æ•°: {len(papers)}")
    print(f"æˆåŠŸä¸‹è½½: {success_count}")
    print(f"å¤±è´¥æ•°é‡: {len(failed_papers)}")
    print(f"æˆåŠŸç‡: {success_count/len(papers)*100:.1f}%")
    
    # 4. ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    results = {
        "total_papers": len(papers),
        "success_count": success_count,
        "failed_count": len(failed_papers),
        "success_rate": success_count/len(papers)*100,
        "papers": [
            {
                "title": paper.title,
                "doi": paper.doi,
                "year": paper.year,
                "authors": paper.authors,
                "journal": paper.journal
            }
            for paper in papers
        ],
        "failed_papers": [
            {
                "title": paper.title,
                "doi": paper.doi,
                "year": paper.year,
                "authors": paper.authors,
                "journal": paper.journal
            }
            for paper in failed_papers
        ]
    }
    
    with open("scihub_test_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: scihub_test_results.json")
    
    if success_count > 0:
        print("âœ… Sci-Hubæ–¹æ¡ˆå¯è¡Œ")
    else:
        print("âŒ Sci-Hubæ–¹æ¡ˆä¸å¯è¡Œ")

if __name__ == "__main__":
    test_scihub_large_scale()
