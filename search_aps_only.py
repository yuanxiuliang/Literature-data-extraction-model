#!/usr/bin/env python3
"""
ä¸“é—¨æœç´¢APSæœŸåˆŠè®ºæ–‡
åªä½¿ç”¨Google Scholaræœç´¢APSæœŸåˆŠï¼Œä¸æœç´¢å…¶ä»–æ¥æº
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.services.advanced_selenium_bypass import AdvancedSeleniumBypass
from app.services.aps_pdf_extractor import APSPDFExtractor
from app.services.pdf_downloader import PDFDownloader
import time

def check_and_handle_captcha(bypass):
    """æ£€æŸ¥å¹¶å¤„ç†äººæœºéªŒè¯"""
    # æ£€æŸ¥æ˜¯å¦é‡åˆ°äººæœºéªŒè¯ - ä½¿ç”¨æ›´ç²¾ç¡®çš„æ£€æµ‹æ¡ä»¶
    page_source = bypass.driver.page_source.lower()
    title = bypass.driver.title.lower()
    
    # æ›´ç²¾ç¡®çš„äººæœºéªŒè¯æ£€æµ‹æ¡ä»¶
    captcha_indicators = [
        "unusual traffic", "suspicious activity", "not a robot",
        "verify you are human", "security check", "captcha",
        "please complete the security check", "robot detection"
    ]
    
    # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«äººæœºéªŒè¯çš„å…³é”®å…ƒç´ 
    is_captcha = False
    for indicator in captcha_indicators:
        if indicator in title or indicator in page_source:
            is_captcha = True
            break
    
    # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯ç ç›¸å…³çš„HTMLå…ƒç´ 
    if not is_captcha:
        try:
            # æ£€æŸ¥å¸¸è§çš„éªŒè¯ç å…ƒç´ 
            captcha_elements = bypass.driver.find_elements("css selector", 
                "iframe[src*='recaptcha'], .g-recaptcha, #captcha, .captcha, [data-callback*='captcha']")
            if captcha_elements:
                is_captcha = True
        except:
            pass
    
    if is_captcha:
        print("ğŸ” æ£€æµ‹åˆ°äººæœºéªŒè¯")
        print("=" * 50)
        print("è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®ŒæˆéªŒè¯ï¼š")
        print("1. åœ¨æµè§ˆå™¨ä¸­å®ŒæˆäººæœºéªŒè¯")
        print("2. ç­‰å¾…éªŒè¯å®Œæˆï¼Œé¡µé¢è·³è½¬åˆ°Google Scholar")
        print("3. æŒ‰å›è½¦é”®ç»§ç»­...")
        print()
        
        input("å®ŒæˆéªŒè¯åæŒ‰å›è½¦é”®ç»§ç»­...")
        
        # é‡æ–°æ£€æŸ¥é¡µé¢çŠ¶æ€
        print("é‡æ–°æ£€æŸ¥é¡µé¢çŠ¶æ€...")
        time.sleep(2)
        print(f"å½“å‰URL: {bypass.driver.current_url}")
        print(f"é¡µé¢æ ‡é¢˜: {bypass.driver.title}")
        
        return True
    else:
        # ä¸æ‰“å°ä»»ä½•ä¿¡æ¯ï¼Œé¿å…è¯¯æŠ¥
        return True

def search_aps_only():
    """ä¸“é—¨æœç´¢APSæœŸåˆŠè®ºæ–‡ - æµ‹è¯•PRBæœŸåˆŠ"""
    print("ä¸“é—¨æœç´¢APSæœŸåˆŠè®ºæ–‡ - æµ‹è¯•PRBæœŸåˆŠ")
    print("=" * 50)
    print("åªä½¿ç”¨Google Scholaræœç´¢APSæœŸåˆŠï¼Œä¸æœç´¢å…¶ä»–æ¥æº")
    print("åŒ…å«äººæœºéªŒè¯å¤„ç†èƒ½åŠ›")
    print()
    print("å½“å‰æµ‹è¯•ï¼šPhysical Review B (PRB) æœŸåˆŠ")
    print("å…¶ä»–æœŸåˆŠå·²æ³¨é‡Šï¼Œæ­£å¼æ£€ç´¢æ—¶ä¼šå¯ç”¨")
    print()
    
    # åˆå§‹åŒ–æœåŠ¡
    bypass = AdvancedSeleniumBypass(headless=False)  # éæ— å¤´æ¨¡å¼ï¼Œä¾¿äºå¤„ç†éªŒè¯
    aps_extractor = APSPDFExtractor(use_selenium=True, use_cloudflare_bypass=True)
    pdf_downloader = PDFDownloader(download_dir="downloads")
    
    try:
        print("æ­¥éª¤1: è®¿é—®Google Scholar")
        print("-" * 30)
        
        # è®¿é—®Google Scholar
        bypass.driver.get("https://scholar.google.com")
        time.sleep(3)
        
        print(f"å½“å‰URL: {bypass.driver.current_url}")
        print(f"é¡µé¢æ ‡é¢˜: {bypass.driver.title}")
        
        # æ£€æŸ¥æ˜¯å¦é‡åˆ°äººæœºéªŒè¯
        if not check_and_handle_captcha(bypass):
            print("âŒ äººæœºéªŒè¯å¤„ç†å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
            return
        
        print("\næ­¥éª¤2: æœç´¢APSæœŸåˆŠè®ºæ–‡")
        print("-" * 30)
        
        # å®šä¹‰APSæœŸåˆŠæœç´¢æŸ¥è¯¢
        aps_search_queries = [
            # "site:journals.aps.org/prl single crystal growth",           # Physical Review Letters - æµ‹è¯•é˜¶æ®µè·³è¿‡
            "site:journals.aps.org/prb single crystal growth",           # Physical Review B - æµ‹è¯•ç›®æ ‡
            # "site:journals.aps.org/prmaterials single crystal growth",   # Physical Review Materials - æµ‹è¯•é˜¶æ®µè·³è¿‡
            # "site:journals.aps.org/prresearch single crystal growth",    # Physical Review Research - æµ‹è¯•é˜¶æ®µè·³è¿‡
            # "site:journals.aps.org/prx single crystal growth",           # Physical Review X - æµ‹è¯•é˜¶æ®µè·³è¿‡
            # "site:journals.aps.org/prapplied single crystal growth"      # Physical Review Applied - æµ‹è¯•é˜¶æ®µè·³è¿‡
        ]
        
        all_aps_papers = []
        
        for query in aps_search_queries:
            print(f"æœç´¢: {query}")
            
            try:
                # é™é»˜æ£€æŸ¥äººæœºéªŒè¯ï¼Œåªåœ¨çœŸæ­£é‡åˆ°æ—¶æ‰æç¤º
                check_and_handle_captcha(bypass)
                
                results = bypass.search_google_scholar(query, max_results=5)
                
                if results:
                    print(f"  âœ… æ‰¾åˆ° {len(results)} ä¸ªAPSè®ºæ–‡")
                    
                    for result in results:
                        # éªŒè¯ç¡®å®æ˜¯APSæœŸåˆŠ
                        if any(domain in result.url for domain in ["journals.aps.org", "aip.scitation.org"]):
                            all_aps_papers.append(result)
                            print(f"    - {result.title}")
                            print(f"      æœŸåˆŠ: {result.journal}")
                            print(f"      URL: {result.url}")
                            print()
                else:
                    print(f"  âŒ æœªæ‰¾åˆ°ç»“æœ")
                
                time.sleep(5)  # å¢åŠ å»¶è¿Ÿï¼Œé¿å…è§¦å‘éªŒè¯
                
            except Exception as e:
                print(f"  âŒ æœç´¢å¤±è´¥: {e}")
                continue
        
        if not all_aps_papers:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•APSæœŸåˆŠè®ºæ–‡")
            return
        
        print(f"\nâœ… æ€»å…±æ‰¾åˆ° {len(all_aps_papers)} ä¸ªAPSæœŸåˆŠè®ºæ–‡")
        
        print("\næ­¥éª¤3: å¤„ç†APSè®ºæ–‡")
        print("-" * 30)
        
        success_count = 0
        for i, paper in enumerate(all_aps_papers[:5], 1):  # å¤„ç†å‰5ä¸ªè®ºæ–‡
            print(f"\nå¤„ç†è®ºæ–‡ {i}: {paper.title}")
            print(f"æœŸåˆŠ: {paper.journal}")
            print(f"URL: {paper.url}")
            
            try:
                # æå–PDFä¿¡æ¯
                print("  æå–PDFä¿¡æ¯...")
                pdf_info = aps_extractor.extract_pdf_info(paper.url)
                
                if pdf_info:
                    print(f"  âœ… PDFä¿¡æ¯æå–æˆåŠŸ")
                    print(f"  PDF URL: {pdf_info.pdf_url}")
                    print(f"  æ ‡é¢˜: {pdf_info.title}")
                    print(f"  ä½œè€…: {pdf_info.authors}")
                    print(f"  æœŸåˆŠ: {pdf_info.journal}")
                    print(f"  è®¿é—®ç±»å‹: {pdf_info.access_type}")
                    
                    # ä¸‹è½½PDF
                    print("  ä¸‹è½½PDF...")
                    filename = f"aps_only_{i}_{pdf_info.title[:30].replace(' ', '_').replace('/', '_')}.pdf"
                    download_result = pdf_downloader.download_pdf(
                        pdf_info.pdf_url, 
                        filename, 
                        pdf_info.access_type
                    )
                    
                    if download_result.success:
                        print(f"  âœ… PDFä¸‹è½½æˆåŠŸ")
                        print(f"  æ–‡ä»¶è·¯å¾„: {download_result.file_path}")
                        print(f"  æ–‡ä»¶å¤§å°: {download_result.file_size} bytes")
                        success_count += 1
                    else:
                        print(f"  âŒ PDFä¸‹è½½å¤±è´¥: {download_result.error}")
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦æƒé™
                        if "403" in str(download_result.error) or "Forbidden" in str(download_result.error):
                            print("  ğŸ” æ£€æµ‹åˆ°æƒé™é—®é¢˜")
                            print("  ğŸ’¡ å»ºè®®ï¼š")
                            print("     1. æ‰‹åŠ¨è®¿é—®: {paper.url}")
                            print("     2. ä¸‹è½½PDFå¹¶ä¿å­˜ä¸º: {filename}")
                            print("     3. å°†æ–‡ä»¶æ”¾å…¥ downloads/ ç›®å½•")
                else:
                    print("  âŒ PDFä¿¡æ¯æå–å¤±è´¥")
                    
            except Exception as e:
                print(f"  âŒ å¤„ç†å¼‚å¸¸: {e}")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print(f"\næ­¥éª¤4: å¤„ç†ç»“æœç»Ÿè®¡")
        print("-" * 30)
        print(f"æ€»å¤„ç†è®ºæ–‡: {min(5, len(all_aps_papers))}")
        print(f"æˆåŠŸå¤„ç†: {success_count}")
        print(f"æˆåŠŸç‡: {success_count/min(5, len(all_aps_papers))*100:.1f}%")
        
        if success_count > 0:
            print("ğŸ‰ APSæœŸåˆŠè®ºæ–‡å¤„ç†å®Œæˆï¼")
        else:
            print("âŒ æ‰€æœ‰APSè®ºæ–‡å¤„ç†å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ å·¥ä½œæµç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        bypass.close()

if __name__ == "__main__":
    search_aps_only()
