#!/usr/bin/env python3
"""
å¯»æ‰¾APSå¼€æ”¾è·å–è®ºæ–‡
é€šè¿‡å¤šç§æ–¹å¼æœç´¢çœŸæ­£çš„APSå¼€æ”¾è·å–è®ºæ–‡è¿›è¡Œæµ‹è¯•
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.services.advanced_selenium_bypass import AdvancedSeleniumBypass
from app.services.anti_crawler_bypass import AntiCrawlerBypass
from app.services.aps_pdf_extractor import APSPDFExtractor
from app.services.pdf_downloader import PDFDownloader
import time

def find_aps_open_access():
    """å¯»æ‰¾APSå¼€æ”¾è·å–è®ºæ–‡"""
    print("å¯»æ‰¾APSå¼€æ”¾è·å–è®ºæ–‡")
    print("=" * 50)
    
    # åˆå§‹åŒ–æœåŠ¡
    anti_crawler = AntiCrawlerBypass()
    aps_extractor = APSPDFExtractor(use_selenium=False)
    pdf_downloader = PDFDownloader(download_dir="downloads")
    
    # æœç´¢ç­–ç•¥
    search_queries = [
        "site:journals.aps.org/prx single crystal growth",
        "site:journals.aps.org/prresearch single crystal growth",
        "site:journals.aps.org/prx quantum crystal",
        "site:journals.aps.org/prresearch flux method",
        "APS Physical Review X open access",
        "APS Physical Review Research open access"
    ]
    
    print("ç­–ç•¥1ï¼šä½¿ç”¨æ›¿ä»£æœç´¢æºæœç´¢APSè®ºæ–‡")
    print("-" * 40)
    
    aps_papers = []
    for query in search_queries:
        print(f"æœç´¢: {query}")
        try:
            results = anti_crawler.search_alternative_sources(query)
            
            for result in results:
                url = result.get('url', '')
                if any(domain in url for domain in ["journals.aps.org", "aip.scitation.org"]):
                    aps_papers.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'authors': ', '.join([author.get('name', '') for author in result.get('authors', [])]),
                        'journal': result.get('venue', ''),
                        'year': result.get('year', ''),
                        'abstract': result.get('abstract', ''),
                        'pdf_url': result.get('pdf_url', '')
                    })
                    print(f"  âœ… æ‰¾åˆ°APSè®ºæ–‡: {result.get('title', '')[:60]}...")
                    print(f"     URL: {url}")
            
            time.sleep(2)  # æ¸©å’Œå»¶è¿Ÿ
        except Exception as e:
            print(f"  âš ï¸ æœç´¢å¤±è´¥: {e}")
            continue
    
    if not aps_papers:
        print("âŒ æ›¿ä»£æœç´¢æºæœªæ‰¾åˆ°APSè®ºæ–‡")
        print("\nç­–ç•¥2ï¼šä½¿ç”¨Google Scholaræœç´¢APSè®ºæ–‡")
        print("-" * 40)
        
        # ä½¿ç”¨Google Scholaræœç´¢
        bypass = AdvancedSeleniumBypass(headless=False)
        try:
            for query in search_queries[:3]:  # åªæµ‹è¯•å‰3ä¸ªæŸ¥è¯¢
                print(f"Google Scholaræœç´¢: {query}")
                results = bypass.search_google_scholar(query, max_results=5)
                
                for result in results:
                    if any(domain in result.url for domain in ["journals.aps.org", "aip.scitation.org"]):
                        aps_papers.append({
                            'title': result.title,
                            'url': result.url,
                            'authors': result.authors,
                            'journal': result.journal,
                            'year': result.year,
                            'abstract': getattr(result, 'abstract', ''),
                            'pdf_url': ''
                        })
                        print(f"  âœ… æ‰¾åˆ°APSè®ºæ–‡: {result.title[:60]}...")
                        print(f"     URL: {result.url}")
                
                time.sleep(3)  # æ¸©å’Œå»¶è¿Ÿ
        except Exception as e:
            print(f"  âŒ Google Scholaræœç´¢å¤±è´¥: {e}")
        finally:
            bypass.close()
    
    if not aps_papers:
        print("âŒ æ‰€æœ‰æœç´¢ç­–ç•¥éƒ½æœªæ‰¾åˆ°APSè®ºæ–‡")
        print("\nç­–ç•¥3ï¼šä½¿ç”¨å·²çŸ¥çš„APSå¼€æ”¾è·å–è®ºæ–‡è¿›è¡Œæµ‹è¯•")
        print("-" * 40)
        
        # ä½¿ç”¨ä¸€äº›å·²çŸ¥çš„APSå¼€æ”¾è·å–è®ºæ–‡URLè¿›è¡Œæµ‹è¯•
        known_aps_papers = [
            {
                'title': 'Physical Review X - Open Access Journal',
                'url': 'https://journals.aps.org/prx/',
                'journal': 'Physical Review X',
                'note': 'è¿™æ˜¯æœŸåˆŠä¸»é¡µï¼Œéœ€è¦æ‰¾åˆ°å…·ä½“è®ºæ–‡'
            },
            {
                'title': 'Physical Review Research - Open Access Journal',
                'url': 'https://journals.aps.org/prresearch/',
                'journal': 'Physical Review Research',
                'note': 'è¿™æ˜¯æœŸåˆŠä¸»é¡µï¼Œéœ€è¦æ‰¾åˆ°å…·ä½“è®ºæ–‡'
            }
        ]
        
        print("å·²çŸ¥çš„APSå¼€æ”¾è·å–æœŸåˆŠ:")
        for paper in known_aps_papers:
            print(f"  - {paper['title']}")
            print(f"    URL: {paper['url']}")
            print(f"    è¯´æ˜: {paper['note']}")
        
        print("\nå»ºè®®:")
        print("1. æ‰‹åŠ¨è®¿é—®è¿™äº›æœŸåˆŠä¸»é¡µï¼ŒæŸ¥æ‰¾å¼€æ”¾è·å–çš„è®ºæ–‡")
        print("2. æˆ–è€…ä½¿ç”¨å…¶ä»–å¼€æ”¾è·å–æœŸåˆŠè¿›è¡Œæµ‹è¯•")
        print("3. æˆ–è€…å…ˆæµ‹è¯•è®¢é˜…æœŸåˆŠçš„è®¿é—®æœºåˆ¶")
        
        return
    
    print(f"\nâœ… æ‰¾åˆ° {len(aps_papers)} ä¸ªAPSè®ºæ–‡")
    print("å¼€å§‹æµ‹è¯•PDFæå–å’Œä¸‹è½½...")
    print("-" * 40)
    
    # æµ‹è¯•å‰3ä¸ªè®ºæ–‡
    for i, paper in enumerate(aps_papers[:3], 1):
        print(f"\næµ‹è¯•è®ºæ–‡ {i}: {paper['title']}")
        print(f"URL: {paper['url']}")
        print(f"æœŸåˆŠ: {paper['journal']}")
        
        try:
            # æå–PDFä¿¡æ¯
            print("æå–PDFä¿¡æ¯...")
            pdf_info = aps_extractor.extract_pdf_info(paper['url'])
            
            if pdf_info:
                print(f"  âœ… PDFä¿¡æ¯æå–æˆåŠŸ")
                print(f"  PDF URL: {pdf_info.pdf_url}")
                print(f"  æ ‡é¢˜: {pdf_info.title}")
                print(f"  ä½œè€…: {pdf_info.authors}")
                print(f"  æœŸåˆŠ: {pdf_info.journal}")
                print(f"  è®¿é—®ç±»å‹: {pdf_info.access_type}")
                
                # ä¸‹è½½PDF
                print("ä¸‹è½½PDF...")
                filename = f"aps_{i}_{pdf_info.title[:30].replace(' ', '_').replace('/', '_')}.pdf"
                download_result = pdf_downloader.download_pdf(
                    pdf_info.pdf_url, 
                    filename, 
                    pdf_info.access_type
                )
                
                if download_result.success:
                    print(f"  âœ… PDFä¸‹è½½æˆåŠŸ")
                    print(f"  æ–‡ä»¶è·¯å¾„: {download_result.file_path}")
                    print(f"  æ–‡ä»¶å¤§å°: {download_result.file_size} bytes")
                    print("  ğŸ‰ æˆåŠŸå®ŒæˆAPS PDFä¸‹è½½æµ‹è¯•ï¼")
                    return
                else:
                    print(f"  âŒ PDFä¸‹è½½å¤±è´¥: {download_result.error}")
                    if "403" in str(download_result.error) or "Forbidden" in str(download_result.error):
                        print("  ğŸ’¡ è¿™å¯èƒ½æ˜¯æƒé™é—®é¢˜ï¼Œéœ€è¦å¼€æ”¾è·å–çš„è®ºæ–‡")
                    elif "404" in str(download_result.error):
                        print("  ğŸ’¡ è¿™å¯èƒ½æ˜¯URLä¸å­˜åœ¨æˆ–PDFé“¾æ¥æ— æ•ˆ")
            else:
                print("  âŒ PDFä¿¡æ¯æå–å¤±è´¥")
                
        except Exception as e:
            print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    print("\nâŒ æ‰€æœ‰APSè®ºæ–‡æµ‹è¯•éƒ½å¤±è´¥äº†")
    print("å»ºè®®:")
    print("1. æ‰‹åŠ¨æŸ¥æ‰¾ä¸€äº›çœŸæ­£çš„APSå¼€æ”¾è·å–è®ºæ–‡URL")
    print("2. æˆ–è€…ä½¿ç”¨å…¶ä»–å¼€æ”¾è·å–æœŸåˆŠè¿›è¡Œæµ‹è¯•")
    print("3. æˆ–è€…å…ˆæµ‹è¯•è®¢é˜…æœŸåˆŠçš„è®¿é—®æœºåˆ¶")

if __name__ == "__main__":
    find_aps_open_access()
