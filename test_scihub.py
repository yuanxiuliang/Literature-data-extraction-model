#!/usr/bin/env python3
"""
Sci-Hubæµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯•Sci-Hubè·å–APSè®ºæ–‡PDFçš„å¯è¡Œæ€§
"""

import requests
import time
import logging
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SciHubTester:
    """Sci-Hubæµ‹è¯•å™¨"""
    
    def __init__(self, base_url="https://sci-hub.in/"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def test_scihub_access(self):
        """æµ‹è¯•Sci-Hubç½‘ç«™è®¿é—®"""
        try:
            logger.info(f"æµ‹è¯•è®¿é—®Sci-Hub: {self.base_url}")
            response = self.session.get(self.base_url, timeout=30)
            
            if response.status_code == 200:
                logger.info("âœ… Sci-Hubç½‘ç«™è®¿é—®æˆåŠŸ")
                return True
            else:
                logger.warning(f"âš ï¸ Sci-Hubè®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Sci-Hubè®¿é—®å¤±è´¥: {e}")
            return False
    
    def search_paper(self, doi):
        """æœç´¢è®ºæ–‡"""
        try:
            logger.info(f"æœç´¢è®ºæ–‡DOI: {doi}")
            
            # æ„å»ºæœç´¢URL
            search_url = f"{self.base_url}{doi}"
            
            # å‘é€è¯·æ±‚
            response = self.session.get(search_url, timeout=30)
            
            if response.status_code == 200:
                logger.info("âœ… è®ºæ–‡æœç´¢æˆåŠŸ")
                return response.text
            else:
                logger.warning(f"âš ï¸ è®ºæ–‡æœç´¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ è®ºæ–‡æœç´¢å¤±è´¥: {e}")
            return None
    
    def extract_pdf_url(self, html_content):
        """ä»HTMLå†…å®¹ä¸­æå–PDFé“¾æ¥"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æŸ¥æ‰¾PDFé“¾æ¥çš„å¤šç§æ–¹å¼
            pdf_selectors = [
                'a[href*=".pdf"]',
                'a[href*="pdf"]',
                'a[title*="PDF"]',
                'a[title*="pdf"]',
                '#pdf',
                '.pdf',
                'a[onclick*="pdf"]'
            ]
            
            pdf_url = None
            for selector in pdf_selectors:
                try:
                    element = soup.select_one(selector)
                    if element and element.get('href'):
                        href = element['href']
                        if href.startswith('//'):
                            pdf_url = 'https:' + href
                        elif href.startswith('/'):
                            pdf_url = self.base_url.rstrip('/') + href
                        elif href.startswith('http'):
                            pdf_url = href
                        else:
                            pdf_url = urljoin(self.base_url, href)
                        break
                except:
                    continue
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»é¡µé¢å†…å®¹ä¸­æœç´¢PDFé“¾æ¥
            if not pdf_url:
                pdf_patterns = [
                    r'href="([^"]*\.pdf[^"]*)"',
                    r'src="([^"]*\.pdf[^"]*)"',
                    r'url\(["\']?([^"\']*\.pdf[^"\']*)["\']?\)'
                ]
                
                for pattern in pdf_patterns:
                    matches = re.findall(pattern, html_content, re.IGNORECASE)
                    if matches:
                        pdf_url = matches[0]
                        if pdf_url.startswith('//'):
                            pdf_url = 'https:' + pdf_url
                        elif pdf_url.startswith('/'):
                            pdf_url = self.base_url.rstrip('/') + pdf_url
                        elif not pdf_url.startswith('http'):
                            pdf_url = urljoin(self.base_url, pdf_url)
                        break
            
            if pdf_url:
                logger.info(f"âœ… æ‰¾åˆ°PDFé“¾æ¥: {pdf_url}")
                return pdf_url
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°PDFé“¾æ¥")
                return None
                
        except Exception as e:
            logger.error(f"âŒ PDFé“¾æ¥æå–å¤±è´¥: {e}")
            return None
    
    def download_pdf(self, pdf_url, filename):
        """ä¸‹è½½PDFæ–‡ä»¶"""
        try:
            logger.info(f"ä¸‹è½½PDF: {pdf_url}")
            
            response = self.session.get(pdf_url, timeout=60, stream=True)
            
            if response.status_code == 200:
                # åˆ›å»ºä¸‹è½½ç›®å½•
                os.makedirs("downloads", exist_ok=True)
                
                # ä¿å­˜æ–‡ä»¶
                filepath = os.path.join("downloads", filename)
                with open(filepath, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                file_size = os.path.getsize(filepath)
                logger.info(f"âœ… PDFä¸‹è½½æˆåŠŸ: {filepath} ({file_size} bytes)")
                return True
            else:
                logger.warning(f"âš ï¸ PDFä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ PDFä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def test_complete_workflow(self, doi, filename):
        """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹"""
        try:
            logger.info(f"å¼€å§‹æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹: {doi}")
            
            # 1. æœç´¢è®ºæ–‡
            html_content = self.search_paper(doi)
            if not html_content:
                return False
            
            # 2. æå–PDFé“¾æ¥
            pdf_url = self.extract_pdf_url(html_content)
            if not pdf_url:
                return False
            
            # 3. ä¸‹è½½PDF
            success = self.download_pdf(pdf_url, filename)
            return success
            
        except Exception as e:
            logger.error(f"âŒ å®Œæ•´å·¥ä½œæµç¨‹å¤±è´¥: {e}")
            return False

def test_scihub():
    """æµ‹è¯•Sci-HubåŠŸèƒ½"""
    print("æµ‹è¯•Sci-Hubè·å–APSè®ºæ–‡PDF")
    print("=" * 50)
    
    tester = SciHubTester()
    
    # 1. æµ‹è¯•ç½‘ç«™è®¿é—®
    print("\næ­¥éª¤1: æµ‹è¯•Sci-Hubç½‘ç«™è®¿é—®")
    print("-" * 30)
    if not tester.test_scihub_access():
        print("âŒ Sci-Hubç½‘ç«™è®¿é—®å¤±è´¥")
        return
    print("âœ… Sci-Hubç½‘ç«™è®¿é—®æˆåŠŸ")
    
    # 2. æµ‹è¯•è®ºæ–‡æœç´¢å’Œä¸‹è½½
    print("\næ­¥éª¤2: æµ‹è¯•è®ºæ–‡æœç´¢å’Œä¸‹è½½")
    print("-" * 30)
    
    # ä½¿ç”¨ä¹‹å‰æ‰¾åˆ°çš„APSè®ºæ–‡DOIè¿›è¡Œæµ‹è¯•
    test_papers = [
        {
            "doi": "10.1103/PhysRevB.64.144524",
            "title": "Crystal growth, transport properties, and crystal structure",
            "filename": "PhysRevB.64.144524.pdf"
        },
        {
            "doi": "10.1103/PhysRevB.82.064404", 
            "title": "Quantum phase transitions in single-crystal",
            "filename": "PhysRevB.82.064404.pdf"
        },
        {
            "doi": "10.1103/PhysRevB.109.214401",
            "title": "Single-crystal growth and characterization",
            "filename": "PhysRevB.109.214401.pdf"
        }
    ]
    
    success_count = 0
    for i, paper in enumerate(test_papers, 1):
        print(f"\næµ‹è¯•è®ºæ–‡ {i}: {paper['title']}")
        print(f"DOI: {paper['doi']}")
        
        if tester.test_complete_workflow(paper['doi'], paper['filename']):
            print("âœ… è®ºæ–‡ä¸‹è½½æˆåŠŸ")
            success_count += 1
        else:
            print("âŒ è®ºæ–‡ä¸‹è½½å¤±è´¥")
        
        # ç­‰å¾…ä¸€ä¸‹å†æµ‹è¯•ä¸‹ä¸€ä¸ª
        time.sleep(2)
    
    print(f"\næµ‹è¯•ç»“æœ: {success_count}/{len(test_papers)} æˆåŠŸ")
    
    if success_count > 0:
        print("âœ… Sci-Hubæ–¹æ¡ˆå¯è¡Œ")
        print("ğŸ“ ä¸‹è½½çš„æ–‡ä»¶ä¿å­˜åœ¨ downloads/ ç›®å½•ä¸­")
    else:
        print("âŒ Sci-Hubæ–¹æ¡ˆä¸å¯è¡Œ")

if __name__ == "__main__":
    test_scihub()
